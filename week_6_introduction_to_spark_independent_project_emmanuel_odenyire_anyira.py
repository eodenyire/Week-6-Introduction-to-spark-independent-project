# -*- coding: utf-8 -*-
"""Week 6 Introduction to Spark: Independent project- Emmanuel Odenyire Anyira.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pM1g73M0TSZxcuvzjnhENSiOKzIEx-I4

#Apache Spark DataFrames Project
##Project Deliverable
You will be required to submit:-
###### A GitHub repository with your project written in Pyspark.
######Instructions:-
*As a Data professional, you need to perform an analysis by answering questions about some stock market data on Safaricom from the years 2012-2017.*
######You will need to perform the following:
######Data Importation and Exploration


> i. Start a spark session and load the stock file while inferring the data types.

> ii. Determine the column names

> iii. Make observations about the schema.

>iv. Show the first 5 rows

>v.  Use the describe method to learn about the data frame

######Data Preparation
>i. Format all the data to 2 decimal places i.e. format_number()

>ii. Create a new data frame with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day

######Data Analysis
>i. What day had the Peak High in Price?

>ii. What is the mean of the Close column?

>iii. What is the max and min of the Volume column?

>iv. How many days was the Close lower than 60 dollars?

>v. What percentage of the time was the High greater than 80 dollars?

>vi.What is the Pearson correlation between High and Volume?

>vii. What is the max High per year?

>viii. What is the average Close for each Calendar Month?

######Data description
● Dataset URL (CSV File): https://bit.ly/3pmchka

Checking validity of the data source by reading the url as a CSV
"""

#To read this CSV into a Python dataframe, you can use the pandas library. 
#Here's the code to do that:

import pandas as pd

url = 'https://bit.ly/3pmchka'
df = pd.read_csv(url)

print(df.head())

"""Pre-requisites"""

# Installing pyspark
# ---
#
!pip install pyspark

# Next, we run a local spark session
# ---
#
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
sc = spark.sparkContext

"""Data Importation and Exploration

Start a spark session and load the stock file while inferring the data types
"""

#Importing the required libraries

from pyspark.sql import SparkSession
from pyspark.sql import SQLContext

# Start a Spark session
spark = SparkSession.builder.appName('SafaricomStockAnalysis').getOrCreate()

# Create SQL context object
sqlCtx = SQLContext(spark.sparkContext)

# Read in the data using the SQL context object
df = sqlCtx.read.csv('saf_stock.csv', header=True, inferSchema=True)

"""Determine the column names

"""

df.columns

"""Make observations about the schema.

"""

df.printSchema()

"""● Show the first 5 rows"""

df.show(5)

"""Use the describe method to learn about the data frame"""

df.describe().show()

"""Data Preparation

Format all the data to 2 decimal places i.e. format_number()

Create a new data frame with a column called HV Ratio that is the ratio of the
High Price versus volume of stock traded for a day
"""

# Import preparation modules
import pyspark.sql.functions as F
from pyspark.sql.functions import lit,when,col,expr,round

# Create a new data frame and round off the columns to two decimal places while adding the new HV column

df_prepared=(
df.withColumn('HV',expr("High/Volume"))#Create the new column HV which is a ratio if High to Volume ratio of stocks traded\
    .withColumn('Open', F.format_number('Open', 2))# Round the Open Column to two decimal places\
    .withColumn('High', F.format_number('High', 2))# Round the high column to two decimal places\
    .withColumn('Low', F.format_number('Low', 2))# Round the Low column to two decimal places\
    .withColumn('Close', F.format_number('Close', 2))# Round the close column to two decimal places\
    .withColumn('Volume', round('Volume', 2))#round the volume column to two decimal places.Round function used in this case to solve comma issues\
    .withColumn('Adj Close', F.format_number('Adj Close', 2))# Round the close column to two decimal places\
    .withColumn('HV',F.format_number('HV', 10)))# Round the new HV column to 10 decimal places
 
df_prepared.show(5)

"""Data Analysis"""

#Register a table in SQL
table = df_prepared.registerTempTable("Data_Analytics")

#Confirm that SAF has been registered
tables = sqlCtx.tableNames()

print(tables)

"""#Question 1: What day had the Peak High in Price?"""

q = "SELECT \
         Date,max(High) AS Peak_High_Price \
     FROM Data_Analytics GROUP BY Date \
    ORDER BY Peak_High_Price DESC LIMIT 1 "
sqlCtx.sql(q).show()

"""#Question 2: What is the mean of the Close column?

"""

q = "\
SELECT\
    MEAN(Close) AS MEAN\
        FROM Data_Analytics"

sqlCtx.sql(q).show()

"""# Question 3:- What is the max and min of the Volume column?

"""

q = "SELECT\
     MIN(Volume) Min_Volume,MAX(Volume) Max_Volume\
         FROM Data_Analytics\
 "

sqlCtx.sql(q).show()

"""#Question 4: -How many days was the Close lower than 60 dollars?

"""

# How many days was the close lower than 60 dollars?
q = "SELECT\
    COUNT(Date)\
    FROM Data_Analytics\
    WHERE Close <= 60\
    "
sqlCtx.sql(q).show()

"""#Question 5: -What percentage of the time was the High greater than 80 dollars?

"""

#What percentage of the time was the High greater than 80 Dollars
# Total column entries were computed earlier. An improved querry of this is to use Common Table Expressions for this querry

q = "SELECT\
     ROUND((COUNT(High)/1258*100),2) Percentage_Greater_Than_80\
         FROM Data_Analytics\
         WHERE High >= 80\
            "

sqlCtx.sql(q).show()

"""#Question 6: - What is the Pearson correlation between High and Volume?

"""

# What is the Pearson correlation between High and Volume

q = "SELECT ROUND(corr(High,Volume),2) Pearson_Correlation\
          FROM Data_Analytics"

sqlCtx.sql(q).show()

"""#Question 7: -What is the max High per year?"""

#What is the Max High per Year

q = "SELECT\
     EXTRACT(YEAR FROM Date) Year,\
     MAX(High) Max_High\
     FROM Data_Analytics\
     GROUP BY Year\
     ORDER BY Max_High DESC"
sqlCtx.sql(q).show()

"""#Question 8: -What is the average Close for each Calendar Month?"""

#What is the average Close for each Calendar Month?
q = "SELECT\
    EXTRACT(MONTH FROM Date) Month,\
    ROUND(AVG(Close),2) Avg_Close\
    FROM Data_Analytics\
    GROUP BY Month\
    ORDER BY Month ASC"

sqlCtx.sql(q).show()

"""Another approach to achieve this with one complete piece of code"""

from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
from pyspark.sql.functions import format_number

# Start a Spark session
spark = SparkSession.builder.appName('SafaricomStockAnalysis').getOrCreate()

# Create SQL context object
sqlCtx = SQLContext(spark.sparkContext)

# Read in the data using the SQL context object
df = sqlCtx.read.csv('saf_stock.csv', header=True, inferSchema=True)

# Determine the column names
print('Column Names:')
for column in df.columns:
    print(column)

# Make observations about the schema
print('Schema:')
df.printSchema()

# Show the first 5 rows
print('First 5 Rows:')
df.show(5)

# Use the describe method to learn about the data frame
print('Dataframe Summary:')
df.describe().show()

# Format all the data to 2 decimal places

df = df.na.fill(0)
numeric_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
for col in numeric_cols:
    df = df.withColumn(col, df[col].cast('double'))

# Create a new data frame with a column called HV Ratio
hv_ratio = df.select((df['High'] / df['Volume']).alias('HV Ratio'))

# What day had the Peak High in Price?
print('Day with Peak High in Price:')
df.orderBy(df['High'].desc()).select('Date').head(1)

# What is the mean of the Close column?
print('Mean of the Close column:')
df.select(mean('Close')).show()

# What is the max and min of the Volume column?
print('Max and Min of the Volume column:')
df.select(max('Volume'), min('Volume')).show()

# How many days was the Close lower than 60 dollars?
print('Days with Close lower than 60 dollars:')
df.filter(df['Close'] < 60).count()

# What percentage of the time was the High greater than 80 dollars?
high_80 = df.filter(df['High'] > 80).count()
total_days = df.count()
percentage_high_80 = high_80 / total_days * 100
print('Percentage of time High was greater than 80 dollars:')
print(percentage_high_80)

# What is the Pearson correlation between High and Volume?
print('Pearson correlation between High and Volume:')
df.select(corr('High', 'Volume')).show()

# What is the max High per year?
max_high = df.select(year('Date').alias('Year'), 'High').groupBy('Year').agg(max('High').alias('Max High'))
print('Max High per year:')
max_high.show()

# What is the average Close for each Calendar Month?
avg_close = df.select(month('Date').alias('Month'), 'Close').groupBy('Month').agg(mean('Close').alias('Avg Close'))
print('Average Close for each Calendar Month:')
avg_close.show()